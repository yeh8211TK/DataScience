{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徵選擇(Feature selection)\n",
    "\n",
    "#### 1. 特徵選擇(feature selection)\n",
    "\n",
    "(1) 特徵選擇(feature selection)的目的\n",
    "\n",
    "    選擇目前已存在的特徵(非創造新的特徵)進行建模(modeling)，以增進模型的表現(model performance)\n",
    "\n",
    "(2) 最佳的特徵子集\n",
    "\n",
    "- 減少過擬合(over fitting)\n",
    "\n",
    "\n",
    "- 改進結果準確度\n",
    "\n",
    "\n",
    "- 增加模型可詮釋性(interpretability)\n",
    "\n",
    "\n",
    "- 減少模型訓練時間\n",
    "\n",
    "\n",
    "#### 2. 特徵選擇(feature selection)的對象與方法\n",
    "\n",
    "- 冗餘特徵(Redundant features)\n",
    "\n",
    "  - 含有重複或不重要資訊的特徵\n",
    "  \n",
    "  - 用來產生統計匯總(aggregate statistic)的數值\n",
    "\n",
    "\n",
    "- 相關特徵(Correlated features)\n",
    "\n",
    "\n",
    "- 特徵工程產生的文字向量(Text vectors)\n",
    "\n",
    "\n",
    "#### 3. 冗餘特徵(redundant feature)\n",
    "\n",
    "    # Create a list of redundant column names to drop\n",
    "    to_drop = [\"col_1\", \"col_2\"]\n",
    "\n",
    "    # Drop those columns from the dataset\n",
    "    df_subset = df.drop(to_drop, axis=1)\n",
    "\n",
    "    # Print out the head of the new dataset\n",
    "    print(df_subset.head())\n",
    "\n",
    "#### 4. 相關特徵(correlated features)\n",
    "\n",
    "(1) 回歸(regression)模型的特徵選擇\n",
    "\n",
    "a. 過濾法(Filter method): 根據統計表現(statistical performance)排序特徵\n",
    "\n",
    "b. Filter functions:\n",
    "\n",
    "- df.corr(): Pearson's correlation matrix\n",
    "\n",
    "\n",
    "- sns.heatmap(corr_object): heatmap plot\n",
    "\n",
    "\n",
    "- abs(): absolute value\n",
    "\n",
    "\n",
    "(2) 使用 df.drop() 移除高度相關的特徵\n",
    "\n",
    "    # Print out the column correlations of the dataset\n",
    "    cor = df.corr()\n",
    "    print(cor)\n",
    "    \n",
    "    # Find the columns where the correlation value are greater than 0.5\n",
    "    to_drop = \"col_names\"\n",
    "\n",
    "    # Drop that column from the DataFrame \n",
    "    df = df.drop(to_drop, axis=1)\n",
    "    \n",
    "(3) 選擇與 output variable 高度相關的特徵\n",
    "\n",
    "    # Print out the column correlations of the dataset\n",
    "    cor = df.corr()\n",
    "    print(cor)\n",
    "\n",
    "    # Correlation matrix heatmap\n",
    "    plt.figure()\n",
    "    sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation with output variable A\n",
    "    cor_target = abs(cor[\"A\"])\n",
    "\n",
    "    # Selecting highly correlated features\n",
    "    best_features = cor_target[cor_target > 0.5]\n",
    "    print(best_features)\n",
    "\n",
    "(4) 統計測試的相關係數(Correlation coefficient):\n",
    "\n",
    "|Feature/Response(output)|Continuous|Categorical|\n",
    "|:-----:|:-----:|:-----:|\n",
    "|Continuous|Pearson's Correlation|LDA|\n",
    "|Categorical|ANOVA|Chi-Square|\n",
    " \n",
    "**Reference:**\n",
    "\n",
    "- [NumPy, SciPy, and Pandas: Correlation With Python](https://realpython.com/numpy-scipy-pandas-correlation-python/)\n",
    "\n",
    "#### 5. 文字向量(text vectors)\n",
    "\n",
    "(1) sklearn: TfidfVectorizer\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    tfidf_vec = TfidfVectorizer()\n",
    "    \n",
    "    text_tfidf = tfidf_vec.fit_transform(df['text'])\n",
    "\n",
    "- 詞彙(vocabulary words): \n",
    "\n",
    "    \n",
    "      tfidf_vec.vocabulary_\n",
    "\n",
    "\n",
    "- 文字權重(weights): \n",
    "\n",
    "\n",
    "      text_tfidf[row].data\n",
    "\n",
    "\n",
    "- 文字指標(indices): \n",
    "\n",
    "\n",
    "      text_tfidf[row].indices\n",
    "\n",
    "(2) 計算文字所對應的權重\n",
    "\n",
    "    # Reverse the key-value pairs in the vocabulary\n",
    "    vocab = {v : k for k, v in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "    def return_weights(vocab, original_vocab, vector, vector_row, top_n):\n",
    "        # Zip together the row indices and weights and pass it into dict function\n",
    "        zipped = dict(zip(vector[vector_row].indices, vector[vector_row].data))\n",
    "        \n",
    "        # Let's transform that zipped dict into a series\n",
    "        zipped_series = pd.Series({vocab[i] : zipped[i] for i in vector[vector_row].indices})\n",
    "        \n",
    "        # Let's sort the series to pull out the top n weighted words\n",
    "        zipped_index = zipped_series.sort_values(ascending=False)[ : top_n].index\n",
    "        \n",
    "        return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "    # Print out the weighted words\n",
    "    print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 0, 5))\n",
    "\n",
    "(3) 以列表收集所有文件中權重值高的文字\n",
    "\n",
    "    def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "        filter_list = []\n",
    "        for i in range(0, vector.shape[0]):\n",
    "            # Here we'll call the return_weights() function and extend the list we're creating\n",
    "            filtered = return_weights(vocab, original_vocab, vector, i, top_n) \n",
    "            filter_list.extend(filtered)\n",
    "            \n",
    "        # Return the list in a set, so we don't get duplicate word indices\n",
    "        return set(filter_list)\n",
    "\n",
    "    # Call the function to get the list of word indices\n",
    "    filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "    # By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "    filtered_text = text_tfidf[:, list(filtered_words)]\n",
    "\n",
    "\n",
    "#### 6. 使用 ML 執行回歸(regression)問題的特徵選擇\n",
    "\n",
    "(1) Wrapper method: Use an ML method to evaluate performance\n",
    "\n",
    "a. Forward selection (LARS-least angle regression)\n",
    "\n",
    "- Starts with no features, adds one at a time\n",
    "\n",
    "Example:\n",
    "\n",
    "    # Import modules (least angle regression with cross-val)\n",
    "    from sklearn.linear_model import LarsCV\n",
    "\n",
    "    # Drop feature suggested not important\n",
    "    X = X.drop('col_not_important', axis=1)\n",
    "\n",
    "    # Instantiate\n",
    "    lars_mod = LarsCV(cv=5, normalize=False)\n",
    "\n",
    "    # Fit\n",
    "    feat_selector = lars_mod.fit(X, y)\n",
    "\n",
    "    # Print r-squared score and estimated alpha (estimated regularization parameter)\n",
    "    print(lars_mod.score(X, y))\n",
    "    print(lars_mod.alpha_)\n",
    "\n",
    "b. Backward elimination\n",
    "\n",
    "- Starts with all features, eliminates one at a time\n",
    "\n",
    "\n",
    "c. Forward selection/backward elimination combination (bidirectional elimination)\n",
    "\n",
    "d. Recursive feature elimination (RFECV)\n",
    "\n",
    "Example: \n",
    "        \n",
    "    # support vector regression estimator\n",
    "    from sklearn.svm import SVR\n",
    "    \n",
    "    # recursive feature elimination with cross-val\n",
    "    from sklearn.feature_selection import RFECV\n",
    "\n",
    "    # Instantiate estimator and feature selector\n",
    "    svr_mod = SVR(kernel=\"linear\")\n",
    "    feat_selector = RFECV(svr_mod, cv=5)\n",
    "\n",
    "    # Fit\n",
    "    feat_selector = feat_selector.fit(X, y)\n",
    "\n",
    "    # Print support(boolean array of selected features) and ranking(feature ranking, selected=1)\n",
    "    print(feat_selector.support_)\n",
    "    print(feat_selector.ranking_)\n",
    "    print(X.columns)\n",
    "\n",
    "(2) Embedded method: Iterative model training to extract features\n",
    "\n",
    "- Ridge (L2)\n",
    "\n",
    "\n",
    "- Lasso (L1)\n",
    "\n",
    "\n",
    "- ElasticNet: L1-ratio regularization which is a combination of L1 and L2.\n",
    "\n",
    "\n",
    "a. regularization functions\n",
    "\n",
    "- Lasso estimator: \n",
    "\n",
    "      sklearn.linear_model.Lasso\n",
    "\n",
    "- Lasso estimator with cross-validation: \n",
    "\n",
    "      sklearn.linear_model.LassoCV\n",
    "\n",
    "- Ridge estimator: \n",
    "\n",
    "      sklearn.linear_model.Ridge\n",
    "\n",
    "- Ridge estimator with cross-validation: \n",
    "\n",
    "      sklearn.linear_model.RidgeCV\n",
    "\n",
    "- ElasticNet estimator: \n",
    "\n",
    "      sklearn.linear_model.ElasticNet\n",
    "\n",
    "- ElasticNet estimator with cross-validation: \n",
    "\n",
    "      sklearn.linear_model.ElasticNetCV\n",
    "\n",
    "b. Examples\n",
    "\n",
    "*Lasso:*\n",
    "\n",
    "    # Import modules\n",
    "    from sklearn.linear_model import Lasso, LassoCV\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.3)\n",
    "\n",
    "    # Instantiate cross-validated lasso, fit\n",
    "    lasso_cv = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "    lasso_cv.fit(X_train, y_train) \n",
    "\n",
    "    # Instantiate a lasso estimator passing the best alpha value from lasso_cv, fit, predict and print MSE\n",
    "    lasso = Lasso(alpha = lasso_cv.alpha_)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    print(mean_squared_error(y_true=y_test, y_pred=lasso.predict(X_test)))\n",
    "\n",
    "*Ridge:* \n",
    "\n",
    "    # Import modules\n",
    "    from sklearn.linear_model import Ridge, RidgeCV\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.3)\n",
    "\n",
    "    # Instantiate cross-validated ridge, fit\n",
    "    ridge_cv = RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "    ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "    # Instantiate ridge, fit, predict and print MSE\n",
    "    ridge = Ridge(alpha = ridge_cv.alpha_)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    print(mean_squared_error(y_true=y_test, y_pred=ridge.predict(X_test)))\n",
    "\n",
    "(3) Feature importance: tree-based ML models\n",
    "\n",
    "a. Random Forest\n",
    "\n",
    "    # Import\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    # Instantiate\n",
    "    rf_mod = RandomForestRegressor(max_depth=2, random_state=123, n_estimators=100, oob_score=True)\n",
    "\n",
    "    # Fit\n",
    "    rf_mod.fit(X, y)\n",
    "\n",
    "    # After model fit\n",
    "    rf_mod.feature_importances_\n",
    "    \n",
    "b. Extra Trees\n",
    "\n",
    "    # Import\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "    # Instantiate\n",
    "    xt_mod = ExtraTreesRegressor()\n",
    "\n",
    "    # Fit\n",
    "    xt_mod.fit(X, y)\n",
    "\n",
    "    # After model fit\n",
    "    xt_mod.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 維度降維(Dimensionality reduction)\n",
    "\n",
    "#### 1. 什麼是維度降維(Dimensionality reduction)?\n",
    "\n",
    "- 非監督式(unsupervised learning)學習方法 (in linear/non-linear fashion)\n",
    "\n",
    "\n",
    "- 組合或分解特徵空間\n",
    "\n",
    "\n",
    "- 藉由特徵擷取(feature extraction)來縮減特徵空間\n",
    "\n",
    "\n",
    "- 線性轉換(Linear transformation)後的特徵空間，特徵彼此無相關(uncorrelated)\n",
    "\n",
    "\n",
    "- 在每個主成份盡可能使變異數(variance)最大\n",
    "\n",
    "\n",
    "#### 2. 維度降維(dimensionality reduction) vs 特徵選擇(feature selection)\n",
    "\n",
    "相同點: 減少資料集的特徵數量\n",
    "\n",
    "相異點:\n",
    "\n",
    "a. 維度降維利用特徵進行線性組合，同時移除多重共線性(multicollinearity)\n",
    "\n",
    "b. 特徵選擇根據特徵與目標變數(target variable)的關係去包含或移除特徵 (無特徵轉換)\n",
    "\n",
    "#### 3. 為什麼要使用維度降維(dimensionality reduction)?\n",
    "\n",
    "(1) 維度災難(curse of dimensionality): 模型表現會隨著特徵的數量增加而降低 (overfitting due to high dimensionality)\n",
    "\n",
    "(2) 使用維度降維(dimensionality reduction)的優點\n",
    "\n",
    "- 加速 ML 的訓練\n",
    "\n",
    "\n",
    "- 可視覺化主成份\n",
    "\n",
    "\n",
    "- 增進模型準確度\n",
    "\n",
    "\n",
    "#### 4. 維度降維的方法\n",
    "\n",
    "(1) 主成分分析(Principal component analysis, PCA)\n",
    "\n",
    "- Relationship between X and y\n",
    "\n",
    "\n",
    "- Calculated by finding principal axes\n",
    "\n",
    "\n",
    "- Translates, rotates and scales data to the direction of the maximum variance\n",
    "\n",
    "\n",
    "- Lower-dimensional projection of the data\n",
    "\n",
    "\n",
    "(2) 奇異值分解(Singular value decomposition, SVD)\n",
    "\n",
    "- Linear algebra and vector calculus\n",
    "\n",
    "\n",
    "- Decomposes data matrix into three matrices\n",
    "\n",
    "\n",
    "- Results in 'singular' values\n",
    "\n",
    "\n",
    "- Variance in data approximately equals the square sum of singular values\n",
    "\n",
    "\n",
    "#### 5. 維度降維的套件\n",
    "\n",
    "- 主成分分析: sklearn.decomposition.PCA\n",
    "\n",
    "\n",
    "- 奇異值分解: sklearn.decomposition.TruncatedSVD\n",
    "\n",
    "\n",
    "- 擬合與轉換資料: PCA/SVD.fit_transform(X)\n",
    "\n",
    "\n",
    "- 主成分(principal components)的可解釋變異(explained variance): PCA/SVD.explained_variance_ratio_\n",
    "\n",
    "\n",
    "Example 1: PCA\n",
    "\n",
    "    # Import module\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    # Feature matrix and target array\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "\n",
    "    # Apply PCA to the dataset (# of components of the input features)\n",
    "    pca = PCA(n_components=3)\n",
    "\n",
    "    # Fit and transform\n",
    "    df_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Look at the percentage of variance explained by the different components\n",
    "    print(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Split the df_pca and the y labels into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_pca, y)\n",
    "\n",
    "    # Fit knn to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Score knn on the test data and print it out\n",
    "    print(knn.score(X_test, y_test))\n",
    "    \n",
    "Example 2: SVD\n",
    "\n",
    "    # Import module\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "    # Feature matrix and target array\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "\n",
    "    # SVD\n",
    "    svd = TruncatedSVD(n_components=3)\n",
    "\n",
    "    # Fit and transform\n",
    "    principalComponents = svd.fit_transform(X)\n",
    "\n",
    "    # Print ratio of variance explained\n",
    "    print(svd.explained_variance_ratio_)\n",
    "\n",
    "#### 6. 維度降維的視覺化\n",
    "\n",
    "(1) 主成分分析(PCA)\n",
    "\n",
    "a. 使用 PCA 分離不同的類別\n",
    "\n",
    "Example:\n",
    "\n",
    "    targets = [0, 1]\n",
    "    colors = ['r', 'b']\n",
    "\n",
    "    # For loop to create plot\n",
    "    for target, color in zip(targets, colors):\n",
    "        indicesToKeep = df_PCA['target'] == target\n",
    "        ax.scatter(df_PCA.loc[indicesToKeep, 'principal component 1'],   \n",
    "                   df_PCA.loc[indicesToKeep, 'principal component 2'],\n",
    "                   c = color, s = 50)\n",
    "    # Legend\n",
    "    ax.legend(targets)\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "\n",
    "b. 使用陡坡圖(scree plot)視覺化主成分(principal components)\n",
    "\n",
    "Example:\n",
    "\n",
    "    # Remove target variable\n",
    "    X = df.drop('target', axis=1)\n",
    "\n",
    "    # Instantiate\n",
    "    pca = PCA(n_components=10)\n",
    "\n",
    "    # Fit and transform\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "\n",
    "    # List principal components names\n",
    "    principal_components = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']\n",
    "\n",
    "    # Create a DataFrame\n",
    "    pca_df = pd.DataFrame({'Variance Explained': pca.explained_variance_ratio_, 'PC':principal_components})\n",
    "\n",
    "    # Plot DataFrame\n",
    "    sns.barplot(x='PC', y='Variance Explained', data=pca_df, color=\"c\")\n",
    "    plt.show()\n",
    "\n",
    "    # Instantiate, fit and transform\n",
    "    pca2 = PCA()\n",
    "    principalComponents2 = pca2.fit_transform(X)\n",
    "\n",
    "    # Assign variance explained\n",
    "    var = pca2.explained_variance_ratio_\n",
    "\n",
    "    # Plot cumulative variance\n",
    "    cumulative_var = np.cumsum(var)*100\n",
    "    plt.plot(cumulative_var,'k-o', markerfacecolor='None', markeredgecolor='k')\n",
    "    plt.title('Principal Component Analysis', fontsize=12)\n",
    "    plt.xlabel(\"Principal Component\", fontsize=12)\n",
    "    plt.ylabel(\"Cumulative Proportion of Variance Explained\", fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "(2) 隨機鄰近嵌入法(t-distributed stochastic neighbor embedding, t-SNE)\n",
    "\n",
    "- 對高維的資料點(pairs of data points)建立機率分佈\n",
    "\n",
    "\n",
    "- 在低維空間嵌入可視化的高維數據(Low-dimensional embedding)\n",
    "\n",
    "\n",
    "TSNE 套件:\n",
    "\n",
    "    # t-sne with data\n",
    "    from sklearn.manifold import TSNE\n",
    "    import seaborn as sns\n",
    "\n",
    "    df = pd.read_csv('dataset.csv')\n",
    "\n",
    "    # Feature matrix\n",
    "    X = df.drop('target', axis=1)\n",
    "\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40)\n",
    "    tsne_results = tsne.fit_transform(X)\n",
    "\n",
    "    df['t-SNE-PC-one'] = tsne_results[:, 0]\n",
    "    df['t-SNE-PC-two'] = tsne_results[:, 1]\n",
    "\n",
    "    # t-sne viz\n",
    "    plt.figure(figsize=(16,10))\n",
    "    sns.scatterplot(x=\"t-SNE-PC-one\", y=\"t-SNE-PC-two\", hue=\"target\", \n",
    "                    palette=sns.color_palette([\"grey\",\"blue\"]),\n",
    "                    data=df, legend=\"full\", alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
